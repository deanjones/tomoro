# ConvFinQA

## Introduction
The task is based on the (ConvFinQA dataset)[https://github.com/czyssrs/ConvFinQA]. 
The requirement is to answer users' questions about financial data using the `train.json` data. 
The approach taken here is to focus on the data stored in the HTML tables and treat the task as a Table QA problem.
This requires creating a structured database that can be used to answer users' questions. Syntactically-correct
SQL DDL and DML statements are generated by an LLM to allow a SQL database to be created from the HTML tables.
An LLM is then used to generate a SQL query based on a user's natural-language question. These tasks are well-suited to LLMs. 

## Requirements

- Python 3 (only tested on 3.13)
- [poetry](https://python-poetry.org/) to manage dependencies ([installation](https://python-poetry.org/docs/#installation)) 
- OpenAI API access (key should be insertted)

## Installation

1. Install dependencies: `poetry install --no-root`
2. Copy `.env_sample` to `.env`
3. Set `OPEN_API_KEY` in `.env`
4. Create the vector store. The vector store is used to identify the set of candidate tables which might be able to answer a user's question.
```
    $ poetry run python -m tomoro.ingest

```
5. Create the sqlite database that is used to answer user queries. There is a python program `tomoro/generate_sql_db.py` which can be used to create
the sqlite DB, but this is quite time-consuming, so a gzipped version is included in this repository which can be unpacked as follows:
```
    $ gunzip convfinqa.sqlite.gz

## Operation

There is a FastAPI-based REST service which is used to generate answers to user's questions. To start this service:
```
    $ poetry run uvicorn tomoro.server:app
```

The service can be queried over http, e.g. using curl:
```
    curl -H "Content-Type: application/json" -X POST -d '{"question": "what is the maximum variance during the quarter ended in september 31 , 2005?"}' http://localhost:8000/query/
```

