{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcddaef-d4ec-4611-9499-3d575805f120",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In the notebook we evaluate the overall performance of the system. The task is to send a natural-language question about some financial data to the system, and for the system to determine the correct answer based on the dataset it has been given.\n",
    "\n",
    "## Metrics\n",
    "\n",
    "The evaluation is a relatively simple determination of accuracy: does the system give the correct answer or not? In the evaluation, there is some leeway granted to the system's response, given that the expected answers often include additional symbols ('%' for percentages, currency symbols for monetary amounts, etc). We have also tried to account for errors introduced by rounding, by assessing a response to be correct if the expected value and actual answer differ by a small amount. It is possible that this introduces some error in the evaluation process.\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "The accuracy of the system is determined to be around 15%. This is low, but not surprising given the difficulty of the task and the lack of time for refinement and tuning of the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d195c91-e3af-427d-9492-f4e65ef796b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d826892-ab87-4c25-9691-725cb27f57fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3037"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the records\n",
    "from tomoro.record import read_records\n",
    "\n",
    "data_path = '../data/train.json'\n",
    "all_records = read_records(data_path)\n",
    "len(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ba56ea-7acf-4d59-bfc4-1cd0d797ca9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# use a random sample of the records\n",
    "records = random.sample(all_records, 100)\n",
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d4681d-adbc-4303-afe6-57fb8d89b4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 129\n"
     ]
    }
   ],
   "source": [
    "from tomoro.utils import make_table_name\n",
    "\n",
    "# the correct answer for the required table name is derived from the record id \n",
    "questions = []\n",
    "expected_answers = []\n",
    "\n",
    "for record in records:\n",
    "    table_name = make_table_name(record.id)\n",
    "    qs = [qa.question for qa in record.qa]\n",
    "    expected_as = [qa.answer for qa in record.qa]\n",
    "    questions.extend(qs)\n",
    "    expected_answers.extend(expected_as)\n",
    "\n",
    "print(len(questions), len(expected_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86307339-e66d-48d2-a9f3-09aef140b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tomoro.vector_store import get_store\n",
    "from tomoro.config import get_env_var\n",
    "from tqdm import tqdm \n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from tomoro.utils import get_llm\n",
    "from tomoro.sql import SQLGenerator\n",
    "from tomoro.retrieve import Retriever\n",
    "\n",
    "store_type = get_env_var('VECTOR_STORE')\n",
    "db_path = '../.vector_db'\n",
    "collection_name = get_env_var('VECTOR_DB_COLLECTION_NAME')\n",
    "vector_store = get_store(store_type, db_path=db_path, collection_name=collection_name)\n",
    "SQL_DB_NAME = get_env_var('SQL_DB_NAME')\n",
    "sqlite_uri = f'sqlite:///../{SQL_DB_NAME}'\n",
    "sql_db = SQLDatabase.from_uri(sqlite_uri)\n",
    "LLM_TYPE = get_env_var('LLM')\n",
    "llm = get_llm(LLM_TYPE)\n",
    "sql_generator = SQLGenerator(sqlite_uri, llm)\n",
    "retriever = Retriever(vector_store, llm, sql_db, sql_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda03d27-c8d7-4a6f-9a04-91b0cf30a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "correct = 0\n",
    "for question, expected_answer in tqdm(zip(questions, expected_answers)):\n",
    "    # print(f'question: {question}')\n",
    "    actual_answer = retriever.retrieve(question, k)\n",
    "    if actual_answer is None or actual_answer == '':\n",
    "        continue\n",
    "    if expected_answer.endswith('%'):\n",
    "        expected_answer = expected_answer[:-1]\n",
    "    if actual_answer.endswith('%'):\n",
    "        actual_answer = actual_answer[:-1]\n",
    "    if expected_answer.startswith('$'):\n",
    "        expected_answer = expected_answer[1:]\n",
    "    if expected_answer==actual_answer:\n",
    "        correct += 1\n",
    "    try:\n",
    "        expected_f = float(expected_answer)\n",
    "        actual_f = float(actual_answer)\n",
    "        if abs(expected_f - actual_f) < 1: # if the float values are close, it's probably just a rounding issue\n",
    "            correct +=1 \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'expected: {expected_answer}, actual: {actual_answer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f028168a-e2e6-4c27-a170-c0e9769f3e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.729%\n"
     ]
    }
   ],
   "source": [
    "accuracy = round(100 * correct / len(questions), 3)\n",
    "print(f'Accuracy: {accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomoro",
   "language": "python",
   "name": "tomoro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
